{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7d9c1c",
   "metadata": {},
   "source": [
    "Notebook header, imports, output dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kero\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 03_train_efficientnet_vit.ipynb\n",
    "# Goal:\n",
    "# - Rebuild datasets & dataloaders from metadata\n",
    "# - Define a shared training loop\n",
    "# - Train EfficientNet and ViT\n",
    "# - Evaluate on test set\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "import torch  # type: ignore\n",
    "import torch.nn as nn  # type: ignore\n",
    "from torch.optim import AdamW  # type: ignore\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR  # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler  # type: ignore\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T  # type: ignore\n",
    "\n",
    "import timm  # type: ignore\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Paths (same as previous notebooks)\n",
    "METADATA_DIR = Path(\"./metadata\")\n",
    "LABEL_MAPPING_PATH = METADATA_DIR / \"label_mapping.json\"\n",
    "DATASET_INDEX_PATH = METADATA_DIR / \"dataset_index.json\"\n",
    "\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e33f85",
   "metadata": {},
   "source": [
    "Check that DataLoaders and num_classes exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "057df30a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LABEL_MAPPING_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 2 – Rebuild num_classes, datasets and dataloaders from JSON\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# --- Load metadata ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mLABEL_MAPPING_PATH\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      5\u001b[39m     label_mapping = json.load(f)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(DATASET_INDEX_PATH, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'LABEL_MAPPING_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 2 – Rebuild num_classes, datasets and dataloaders from JSON\n",
    "\n",
    "# --- Load metadata ---\n",
    "with open(LABEL_MAPPING_PATH, \"r\") as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "with open(DATASET_INDEX_PATH, \"r\") as f:\n",
    "    dataset_index = json.load(f)\n",
    "\n",
    "num_classes = len(label_mapping[\"classes\"])\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Number of samples in dataset_index:\", len(dataset_index))\n",
    "\n",
    "# Helper maps\n",
    "id_to_label = {c[\"id\"]: c[\"canonical_label\"] for c in label_mapping[\"classes\"]}\n",
    "label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "\n",
    "# --- Field-poor classes (for augmentations) ---\n",
    "FIELD_POOR_THRESHOLD = 5\n",
    "field_count_by_class = {\n",
    "    c[\"id\"]: c.get(\"field_count\", 0)\n",
    "    for c in label_mapping[\"classes\"]\n",
    "}\n",
    "field_poor_classes = {\n",
    "    cid for cid, cnt in field_count_by_class.items()\n",
    "    if cnt <= FIELD_POOR_THRESHOLD\n",
    "}\n",
    "print(\"Field-poor classes:\", len(field_poor_classes))\n",
    "\n",
    "# --- Transforms (same logic as notebook 2) ---\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "IMG_SIZE = 224\n",
    "\n",
    "transform_pv_basic = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "transform_pv_field_style = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=20),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1),\n",
    "    T.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.8, 1.2)\n",
    "    ),\n",
    "    T.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "transform_field = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "transform_eval = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# --- Dataset class (same as notebook 2) ---\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, entries, transform_train=True):\n",
    "        self.entries = entries\n",
    "        self.transform_train = transform_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.entries[idx]\n",
    "        img_path = item[\"path\"]\n",
    "        class_id = item[\"class_id\"]\n",
    "        domain = item.get(\"domain\", \"pv\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform_train:\n",
    "            if domain == \"field\":\n",
    "                img = transform_field(img)\n",
    "            elif domain == \"pv\":\n",
    "                if class_id in field_poor_classes and torch.rand(1).item() < 0.5:\n",
    "                    img = transform_pv_field_style(img)\n",
    "                else:\n",
    "                    img = transform_pv_basic(img)\n",
    "            else:\n",
    "                img = transform_pv_field_style(img)\n",
    "        else:\n",
    "            img = transform_eval(img)\n",
    "\n",
    "        return img, class_id\n",
    "\n",
    "# --- Split entries into train/val/test ---\n",
    "train_entries = [e for e in dataset_index if e[\"split\"] == \"train\"]\n",
    "val_entries   = [e for e in dataset_index if e[\"split\"] == \"val\"]\n",
    "test_entries  = [e for e in dataset_index if e[\"split\"] == \"test\"]\n",
    "\n",
    "print(\"Train:\", len(train_entries), \"Val:\", len(val_entries), \"Test:\", len(test_entries))\n",
    "\n",
    "train_dataset = PlantDataset(train_entries, transform_train=True)\n",
    "val_dataset   = PlantDataset(val_entries,   transform_train=False)\n",
    "test_dataset  = PlantDataset(test_entries,  transform_train=False)\n",
    "\n",
    "# --- WeightedRandomSampler for class balancing ---\n",
    "train_class_counts = Counter(e[\"class_id\"] for e in train_entries)\n",
    "max_count = max(train_class_counts.values())\n",
    "class_weights = {cid: max_count / cnt for cid, cnt in train_class_counts.items()}\n",
    "sample_weights = [class_weights[e[\"class_id\"]] for e in train_entries]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=torch.DoubleTensor(sample_weights),\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Batches -> Train:\", len(train_loader), \"Val:\", len(val_loader), \"Test:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6b651",
   "metadata": {},
   "source": [
    "Helper: training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5508bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = (all_targets == all_preds).mean()\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = (all_targets == all_preds).mean()\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1, all_targets, all_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ceebe",
   "metadata": {},
   "source": [
    "Helper: create model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20eea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optim(model_name, num_classes, lr=3e-4, weight_decay=1e-4, device=DEVICE):\n",
    "    \"\"\"\n",
    "    model_name: any timm model, e.g. 'efficientnet_b0', 'vit_base_patch16_224'\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=True,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # You can also set different learning rates for backbone/head if you want.\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Set T_max to something like number of epochs\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, criterion, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8e0d5",
   "metadata": {},
   "source": [
    "Generic training loop for one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53438b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_name,\n",
    "    num_classes,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    max_epochs=20,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    device=DEVICE,\n",
    "    early_stopping_patience=5\n",
    "):\n",
    "    print(f\"Starting training for model: {model_name}\")\n",
    "\n",
    "    model, criterion, optimizer, scheduler = create_model_and_optim(\n",
    "        model_name=model_name,\n",
    "        num_classes=num_classes,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_state = None\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": []\n",
    "    }\n",
    "\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, _, _ = evaluate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_f1\"].append(train_f1)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train loss: {train_loss:.4f}, acc: {train_acc:.3f}, F1: {train_f1:.3f} | \"\n",
    "            f\"Val loss: {val_loss:.4f}, acc: {val_acc:.3f}, F1: {val_f1:.3f} | \"\n",
    "            f\"time: {elapsed:.1f}s\"\n",
    "        )\n",
    "\n",
    "        # Save best model by validation macro F1\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"New best val F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Restore best weights\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    ckpt_path = MODELS_DIR / f\"{model_name}_best.pt\"\n",
    "    torch.save({\n",
    "        \"model_name\": model_name,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"num_classes\": num_classes,\n",
    "        \"best_val_f1\": best_val_f1,\n",
    "        \"history\": history\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved best checkpoint to: {ckpt_path}\")\n",
    "\n",
    "    return model, history, best_val_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca53e2",
   "metadata": {},
   "source": [
    "Train EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88258247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for model: efficientnet_b0\n"
     ]
    }
   ],
   "source": [
    "# Example: EfficientNet-B0\n",
    "eff_name = \"efficientnet_b0\"\n",
    "\n",
    "model_eff, history_eff, best_val_f1_eff = train_model(\n",
    "    model_name=eff_name,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_epochs=20,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    device=DEVICE,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"Best validation F1 (EfficientNet-B0):\", best_val_f1_eff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8cdbd6",
   "metadata": {},
   "source": [
    "Evaluate EfficientNet on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51bd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss_eff, test_acc_eff, test_f1_eff, y_true_eff, y_pred_eff = evaluate(\n",
    "    model_eff, test_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "print(f\"EfficientNet-B0 test loss: {test_loss_eff:.4f}\")\n",
    "print(f\"EfficientNet-B0 test acc:  {test_acc_eff:.3f}\")\n",
    "print(f\"EfficientNet-B0 test F1:   {test_f1_eff:.3f}\")\n",
    "\n",
    "print(\"\\nClassification report (EfficientNet-B0):\")\n",
    "print(classification_report(y_true_eff, y_pred_eff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eccccc",
   "metadata": {},
   "source": [
    "Train ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa61816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ViT Base 16x16 patch, 224 input\n",
    "vit_name = \"vit_base_patch16_224\"\n",
    "\n",
    "model_vit, history_vit, best_val_f1_vit = train_model(\n",
    "    model_name=vit_name,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_epochs=20,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    device=DEVICE,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"Best validation F1 (ViT):\", best_val_f1_vit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680cb3b",
   "metadata": {},
   "source": [
    "Evaluate ViT on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss_vit, test_acc_vit, test_f1_vit, y_true_vit, y_pred_vit = evaluate(\n",
    "    model_vit, test_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "print(f\"ViT test loss: {test_loss_vit:.4f}\")\n",
    "print(f\"ViT test acc:  {test_acc_vit:.3f}\")\n",
    "print(f\"ViT test F1:   {test_f1_vit:.3f}\")\n",
    "\n",
    "print(\"\\nClassification report (ViT):\")\n",
    "print(classification_report(y_true_vit, y_pred_vit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f05c9",
   "metadata": {},
   "source": [
    "Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history, title_prefix=\"Model\"):\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"val\")\n",
    "    plt.title(f\"{title_prefix} loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"val\")\n",
    "    plt.title(f\"{title_prefix} accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # F1\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, history[\"train_f1\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_f1\"], label=\"val\")\n",
    "    plt.title(f\"{title_prefix} macro F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_eff, title_prefix=\"EfficientNet-B0\")\n",
    "plot_history(history_vit, title_prefix=\"ViT-B16\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
