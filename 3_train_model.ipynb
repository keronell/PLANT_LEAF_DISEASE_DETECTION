{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7d9c1c",
   "metadata": {},
   "source": [
    "Notebook header, imports, output dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1449c74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 03_train_efficientnet_vit.ipynb\n",
    "# Goal:\n",
    "# - Rebuild datasets & dataloaders from metadata\n",
    "# - Define a shared training loop\n",
    "# - Train EfficientNet and ViT\n",
    "# - Evaluate on test set\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import timm\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Paths (same as previous notebooks)\n",
    "METADATA_DIR = Path(\"./metadata\")\n",
    "LABEL_MAPPING_PATH = METADATA_DIR / \"label_mapping.json\"\n",
    "DATASET_INDEX_PATH = METADATA_DIR / \"dataset_index.json\"\n",
    "\n",
    "MODELS_DIR = Path(\"./models\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e33f85",
   "metadata": {},
   "source": [
    "Check that DataLoaders and num_classes exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "057df30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 39\n",
      "Number of samples in dataset_index: 55448\n",
      "Field-poor classes: 39\n",
      "Train: 44343 Val: 5542 Test: 5563\n",
      "Batches -> Train: 1386 Val: 174 Test: 174\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€“ Rebuild num_classes, datasets and dataloaders from JSON\n",
    "\n",
    "# --- Load metadata ---\n",
    "with open(LABEL_MAPPING_PATH, \"r\") as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "with open(DATASET_INDEX_PATH, \"r\") as f:\n",
    "    dataset_index = json.load(f)\n",
    "\n",
    "num_classes = len(label_mapping[\"classes\"])\n",
    "print(\"Number of classes:\", num_classes)\n",
    "print(\"Number of samples in dataset_index:\", len(dataset_index))\n",
    "\n",
    "# Helper maps\n",
    "id_to_label = {c[\"id\"]: c[\"canonical_label\"] for c in label_mapping[\"classes\"]}\n",
    "label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "\n",
    "# --- Field-poor classes (for augmentations) ---\n",
    "FIELD_POOR_THRESHOLD = 5\n",
    "field_count_by_class = {\n",
    "    c[\"id\"]: c.get(\"field_count\", 0)\n",
    "    for c in label_mapping[\"classes\"]\n",
    "}\n",
    "field_poor_classes = {\n",
    "    cid for cid, cnt in field_count_by_class.items()\n",
    "    if cnt <= FIELD_POOR_THRESHOLD\n",
    "}\n",
    "print(\"Field-poor classes:\", len(field_poor_classes))\n",
    "\n",
    "# --- Transforms (same logic as notebook 2) ---\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "IMG_SIZE = 224\n",
    "\n",
    "transform_pv_basic = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "transform_pv_field_style = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomRotation(degrees=20),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1),\n",
    "    T.RandomAffine(\n",
    "        degrees=0,\n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.8, 1.2)\n",
    "    ),\n",
    "    T.ToTensor(),\n",
    "    T.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "transform_field = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "transform_eval = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# --- Dataset class (same as notebook 2) ---\n",
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, entries, transform_train=True):\n",
    "        self.entries = entries\n",
    "        self.transform_train = transform_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.entries[idx]\n",
    "        img_path = item[\"path\"]\n",
    "        class_id = item[\"class_id\"]\n",
    "        domain = item.get(\"domain\", \"pv\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform_train:\n",
    "            if domain == \"field\":\n",
    "                img = transform_field(img)\n",
    "            elif domain == \"pv\":\n",
    "                if class_id in field_poor_classes and torch.rand(1).item() < 0.5:\n",
    "                    img = transform_pv_field_style(img)\n",
    "                else:\n",
    "                    img = transform_pv_basic(img)\n",
    "            else:\n",
    "                img = transform_pv_field_style(img)\n",
    "        else:\n",
    "            img = transform_eval(img)\n",
    "\n",
    "        return img, class_id\n",
    "\n",
    "# --- Split entries into train/val/test ---\n",
    "train_entries = [e for e in dataset_index if e[\"split\"] == \"train\"]\n",
    "val_entries   = [e for e in dataset_index if e[\"split\"] == \"val\"]\n",
    "test_entries  = [e for e in dataset_index if e[\"split\"] == \"test\"]\n",
    "\n",
    "print(\"Train:\", len(train_entries), \"Val:\", len(val_entries), \"Test:\", len(test_entries))\n",
    "\n",
    "train_dataset = PlantDataset(train_entries, transform_train=True)\n",
    "val_dataset   = PlantDataset(val_entries,   transform_train=False)\n",
    "test_dataset  = PlantDataset(test_entries,  transform_train=False)\n",
    "\n",
    "# --- WeightedRandomSampler for class balancing ---\n",
    "train_class_counts = Counter(e[\"class_id\"] for e in train_entries)\n",
    "max_count = max(train_class_counts.values())\n",
    "class_weights = {cid: max_count / cnt for cid, cnt in train_class_counts.items()}\n",
    "sample_weights = [class_weights[e[\"class_id\"]] for e in train_entries]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=torch.DoubleTensor(sample_weights),\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"Batches -> Train:\", len(train_loader), \"Val:\", len(val_loader), \"Test:\", len(test_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f6b651",
   "metadata": {},
   "source": [
    "Helper: training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5508bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    for batch_idx, (images, targets) in enumerate(loader):\n",
    "        # DEBUG: print every 50 batches\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"  [train] batch {batch_idx}/{len(loader)}\")\n",
    "\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = (all_targets == all_preds).mean()\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = (all_targets == all_preds).mean()\n",
    "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_f1, all_targets, all_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726ceebe",
   "metadata": {},
   "source": [
    "Helper: create model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d20eea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_optim(model_name, num_classes, lr=3e-4, weight_decay=1e-4, device=DEVICE):\n",
    "    \"\"\"\n",
    "    model_name: any timm model, e.g. 'efficientnet_b0', 'vit_base_patch16_224'\n",
    "    \"\"\"\n",
    "    model = timm.create_model(\n",
    "        model_name,\n",
    "        pretrained=False,    # keep False for now so it doesn't try to download weights\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, criterion, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8e0d5",
   "metadata": {},
   "source": [
    "Generic training loop for one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e53438b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_name,\n",
    "    num_classes,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    max_epochs=20,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    device=DEVICE,\n",
    "    early_stopping_patience=5\n",
    "):\n",
    "    print(f\"Starting training for model: {model_name}\")\n",
    "    print(\"  -> Creating model and optimizer...\")\n",
    "\n",
    "    model, criterion, optimizer, scheduler = create_model_and_optim(\n",
    "        model_name=model_name,\n",
    "        num_classes=num_classes,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(\"  -> Model created, starting epochs...\")\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    best_state = None\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"train_f1\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": []\n",
    "    }\n",
    "\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        val_loss, val_acc, val_f1, _, _ = evaluate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_f1\"].append(train_f1)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"Train loss: {train_loss:.4f}, acc: {train_acc:.3f}, F1: {train_f1:.3f} | \"\n",
    "            f\"Val loss: {val_loss:.4f}, acc: {val_acc:.3f}, F1: {val_f1:.3f} | \"\n",
    "            f\"time: {elapsed:.1f}s\"\n",
    "        )\n",
    "\n",
    "        # Track best by validation macro F1\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"New best val F1: {best_val_f1:.4f}\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Restore best weights if we have them\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    ckpt_path = MODELS_DIR / f\"{model_name}_best.pt\"\n",
    "    torch.save({\n",
    "        \"model_name\": model_name,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"num_classes\": num_classes,\n",
    "        \"best_val_f1\": best_val_f1,\n",
    "        \"history\": history\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved best checkpoint to: {ckpt_path}\")\n",
    "\n",
    "    # IMPORTANT: actually return values\n",
    "    return model, history, best_val_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca53e2",
   "metadata": {},
   "source": [
    "Train EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88258247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for model: efficientnet_b0\n",
      "  -> Creating model and optimizer...\n",
      "  -> Model created, starting epochs...\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 01 | Train loss: 2.1528, acc: 0.401, F1: 0.395 | Val loss: 0.6751, acc: 0.790, F1: 0.734 | time: 377.4s\n",
      "New best val F1: 0.7344\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 02 | Train loss: 0.8829, acc: 0.725, F1: 0.723 | Val loss: 0.3726, acc: 0.876, F1: 0.839 | time: 243.9s\n",
      "New best val F1: 0.8395\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 03 | Train loss: 0.5441, acc: 0.826, F1: 0.825 | Val loss: 0.2463, acc: 0.917, F1: 0.899 | time: 221.4s\n",
      "New best val F1: 0.8989\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 04 | Train loss: 0.3954, acc: 0.872, F1: 0.871 | Val loss: 0.2164, acc: 0.925, F1: 0.916 | time: 225.9s\n",
      "New best val F1: 0.9160\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 05 | Train loss: 0.3067, acc: 0.899, F1: 0.899 | Val loss: 0.1724, acc: 0.944, F1: 0.930 | time: 375.0s\n",
      "New best val F1: 0.9301\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 06 | Train loss: 0.2404, acc: 0.922, F1: 0.922 | Val loss: 0.1651, acc: 0.946, F1: 0.931 | time: 252.7s\n",
      "New best val F1: 0.9313\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 07 | Train loss: 0.1985, acc: 0.934, F1: 0.934 | Val loss: 0.1267, acc: 0.958, F1: 0.950 | time: 201.4s\n",
      "New best val F1: 0.9505\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 08 | Train loss: 0.1703, acc: 0.944, F1: 0.944 | Val loss: 0.0941, acc: 0.968, F1: 0.961 | time: 199.9s\n",
      "New best val F1: 0.9615\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 09 | Train loss: 0.1424, acc: 0.953, F1: 0.953 | Val loss: 0.0824, acc: 0.975, F1: 0.970 | time: 298.5s\n",
      "New best val F1: 0.9703\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 10 | Train loss: 0.1183, acc: 0.961, F1: 0.960 | Val loss: 0.0837, acc: 0.973, F1: 0.972 | time: 405.0s\n",
      "New best val F1: 0.9717\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 11 | Train loss: 0.1022, acc: 0.966, F1: 0.966 | Val loss: 0.0574, acc: 0.981, F1: 0.978 | time: 412.9s\n",
      "New best val F1: 0.9781\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 12 | Train loss: 0.0864, acc: 0.971, F1: 0.971 | Val loss: 0.0508, acc: 0.982, F1: 0.978 | time: 405.5s\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 13 | Train loss: 0.0724, acc: 0.976, F1: 0.976 | Val loss: 0.0439, acc: 0.986, F1: 0.983 | time: 432.7s\n",
      "New best val F1: 0.9828\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 14 | Train loss: 0.0639, acc: 0.979, F1: 0.979 | Val loss: 0.0353, acc: 0.988, F1: 0.985 | time: 431.9s\n",
      "New best val F1: 0.9855\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 15 | Train loss: 0.0540, acc: 0.983, F1: 0.983 | Val loss: 0.0318, acc: 0.990, F1: 0.987 | time: 428.2s\n",
      "New best val F1: 0.9873\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 16 | Train loss: 0.0460, acc: 0.985, F1: 0.985 | Val loss: 0.0285, acc: 0.990, F1: 0.988 | time: 401.8s\n",
      "New best val F1: 0.9883\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 17 | Train loss: 0.0399, acc: 0.987, F1: 0.987 | Val loss: 0.0268, acc: 0.991, F1: 0.989 | time: 428.1s\n",
      "New best val F1: 0.9892\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 18 | Train loss: 0.0352, acc: 0.988, F1: 0.988 | Val loss: 0.0243, acc: 0.991, F1: 0.989 | time: 403.5s\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 19 | Train loss: 0.0351, acc: 0.988, F1: 0.988 | Val loss: 0.0240, acc: 0.992, F1: 0.990 | time: 395.7s\n",
      "New best val F1: 0.9898\n",
      "  [train] batch 0/1386\n",
      "  [train] batch 50/1386\n",
      "  [train] batch 100/1386\n",
      "  [train] batch 150/1386\n",
      "  [train] batch 200/1386\n",
      "  [train] batch 250/1386\n",
      "  [train] batch 300/1386\n",
      "  [train] batch 350/1386\n",
      "  [train] batch 400/1386\n",
      "  [train] batch 450/1386\n",
      "  [train] batch 500/1386\n",
      "  [train] batch 550/1386\n",
      "  [train] batch 600/1386\n",
      "  [train] batch 650/1386\n",
      "  [train] batch 700/1386\n",
      "  [train] batch 750/1386\n",
      "  [train] batch 800/1386\n",
      "  [train] batch 850/1386\n",
      "  [train] batch 900/1386\n",
      "  [train] batch 950/1386\n",
      "  [train] batch 1000/1386\n",
      "  [train] batch 1050/1386\n",
      "  [train] batch 1100/1386\n",
      "  [train] batch 1150/1386\n",
      "  [train] batch 1200/1386\n",
      "  [train] batch 1250/1386\n",
      "  [train] batch 1300/1386\n",
      "  [train] batch 1350/1386\n",
      "Epoch 20 | Train loss: 0.0336, acc: 0.989, F1: 0.989 | Val loss: 0.0232, acc: 0.992, F1: 0.991 | time: 376.0s\n",
      "New best val F1: 0.9905\n",
      "Saved best checkpoint to: models\\efficientnet_b0_best.pt\n",
      "Best validation F1 (EfficientNet-B0): 0.9905223583609916\n"
     ]
    }
   ],
   "source": [
    "eff_name = \"efficientnet_b0\"\n",
    "\n",
    "model_eff, history_eff, best_val_f1_eff = train_model(\n",
    "    model_name=eff_name,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_epochs=20,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    device=DEVICE,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"Best validation F1 (EfficientNet-B0):\", best_val_f1_eff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8cdbd6",
   "metadata": {},
   "source": [
    "Evaluate EfficientNet on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d51bd92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet-B0 test loss: 0.0218\n",
      "EfficientNet-B0 test acc:  0.994\n",
      "EfficientNet-B0 test F1:   0.993\n",
      "\n",
      "Classification report (EfficientNet-B0):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        63\n",
      "           1       1.00      1.00      1.00        63\n",
      "           2       1.00      1.00      1.00        28\n",
      "           3       0.98      0.99      0.98       165\n",
      "           4       1.00      1.00      1.00       115\n",
      "           5       0.99      1.00      1.00       151\n",
      "           6       1.00      1.00      1.00        86\n",
      "           7       1.00      1.00      1.00       106\n",
      "           8       1.00      0.94      0.97        52\n",
      "           9       1.00      1.00      1.00       120\n",
      "          10       0.99      1.00      1.00       117\n",
      "          11       0.97      0.99      0.98        99\n",
      "          12       0.98      1.00      0.99       118\n",
      "          13       1.00      0.99      0.99       139\n",
      "          14       1.00      1.00      1.00        43\n",
      "          15       1.00      1.00      1.00       108\n",
      "          16       1.00      1.00      1.00       551\n",
      "          17       1.00      1.00      1.00       230\n",
      "          18       1.00      1.00      1.00        36\n",
      "          19       1.00      0.98      0.99       100\n",
      "          20       0.99      1.00      1.00       148\n",
      "          21       0.98      1.00      0.99       100\n",
      "          22       1.00      1.00      1.00        16\n",
      "          23       0.96      0.98      0.97       100\n",
      "          24       1.00      1.00      1.00        38\n",
      "          25       1.00      1.00      1.00       509\n",
      "          26       1.00      1.00      1.00       184\n",
      "          27       1.00      1.00      1.00        46\n",
      "          28       1.00      1.00      1.00       111\n",
      "          29       1.00      1.00      1.00       213\n",
      "          30       0.98      0.96      0.97       100\n",
      "          31       0.99      1.00      0.99       160\n",
      "          32       0.99      0.96      0.98       191\n",
      "          33       0.97      1.00      0.98        96\n",
      "          34       0.99      1.00      1.00       178\n",
      "          35       0.98      0.98      0.98       168\n",
      "          36       0.98      0.99      0.98       141\n",
      "          37       1.00      1.00      1.00        38\n",
      "          38       1.00      1.00      1.00       536\n",
      "\n",
      "    accuracy                           0.99      5563\n",
      "   macro avg       0.99      0.99      0.99      5563\n",
      "weighted avg       0.99      0.99      0.99      5563\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss_eff, test_acc_eff, test_f1_eff, y_true_eff, y_pred_eff = evaluate(\n",
    "    model_eff, test_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "print(f\"EfficientNet-B0 test loss: {test_loss_eff:.4f}\")\n",
    "print(f\"EfficientNet-B0 test acc:  {test_acc_eff:.3f}\")\n",
    "print(f\"EfficientNet-B0 test F1:   {test_f1_eff:.3f}\")\n",
    "\n",
    "print(\"\\nClassification report (EfficientNet-B0):\")\n",
    "print(classification_report(y_true_eff, y_pred_eff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eccccc",
   "metadata": {},
   "source": [
    "Train ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa61816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: ViT Base 16x16 patch, 224 input\n",
    "vit_name = \"vit_base_patch16_224\"\n",
    "\n",
    "model_vit, history_vit, best_val_f1_vit = train_model(\n",
    "    model_name=vit_name,\n",
    "    num_classes=num_classes,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_epochs=20,\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-4,\n",
    "    device=DEVICE,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"Best validation F1 (ViT):\", best_val_f1_vit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680cb3b",
   "metadata": {},
   "source": [
    "Evaluate ViT on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "test_loss_vit, test_acc_vit, test_f1_vit, y_true_vit, y_pred_vit = evaluate(\n",
    "    model_vit, test_loader, criterion, DEVICE\n",
    ")\n",
    "\n",
    "print(f\"ViT test loss: {test_loss_vit:.4f}\")\n",
    "print(f\"ViT test acc:  {test_acc_vit:.3f}\")\n",
    "print(f\"ViT test F1:   {test_f1_vit:.3f}\")\n",
    "\n",
    "print(\"\\nClassification report (ViT):\")\n",
    "print(classification_report(y_true_vit, y_pred_vit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f05c9",
   "metadata": {},
   "source": [
    "Plot training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history, title_prefix=\"Model\"):\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    # Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"val\")\n",
    "    plt.title(f\"{title_prefix} loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"val\")\n",
    "    plt.title(f\"{title_prefix} accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    # F1\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, history[\"train_f1\"], label=\"train\")\n",
    "    plt.plot(epochs, history[\"val_f1\"], label=\"val\")\n",
    "    plt.title(f\"{title_prefix} macro F1\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_eff, title_prefix=\"EfficientNet-B0\")\n",
    "plot_history(history_vit, title_prefix=\"ViT-B16\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
